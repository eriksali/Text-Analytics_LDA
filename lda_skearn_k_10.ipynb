{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONMrzGmyKPdUmAFrNo0M0W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eriksali/Text-Analytics_LDA/blob/master/lda_skearn_k_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F_EKZrckedD8"
      },
      "outputs": [],
      "source": [
        "# for text preprocessing\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# import vectorizers\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "# import numpy for matrix operation\n",
        "import numpy as np\n",
        "\n",
        "# import LDA from sklearn\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to suppress warnings\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "6EbIxxeKesMY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "dDGSj1oMe1AH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D1 =  'prepar execut eval client eval client client list client proxi fail client success client arr array arr serial ndarray byte arr evalu evalu paramet tensor arr serial tensor type client client result failur evalu client client len result len failur result loss result num exampl'\n",
        "D2 =  'img draw creat imag origin img creat imag output img origin img copi input1 output img draw run input1 origin img shape output img shape rais error equal origin img output img equal output img array equal output img array'\n",
        "D3 =  'cli logger init src custom node init default parent dir cwd case log captur result cli runner invok cli result exit code captur record get messag cwd parent dir exist parent dir exist cwd exist open cwd infil case dict equal yaml safe load infil'\n",
        "D4 =  'record collect record copi deepcopi record record copi get record record record sum copi record len record sum len record'\n",
        "D5 =  'rmse correct pred torch tensor repeat targ torch tensor repeat loss equal pred targ' "
      ],
      "metadata": {
        "id": "GyhBfGNee9W4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combining all the documents into a list:\n",
        "\n",
        "corpus = [D1, D2, D3, D4, D5]"
      ],
      "metadata": {
        "id": "rWG3xJjKfBYt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the complete corpus as below:\n",
        "\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqjyradTfCCV",
        "outputId": "27aff8b0-8f4b-4c9b-9d48-39a5dcd10168"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['prepar execut eval client eval client client list client proxi fail client success client arr array arr serial ndarray byte arr evalu evalu paramet tensor arr serial tensor type client client result failur evalu client client len result len failur result loss result num exampl',\n",
              " 'img draw creat imag origin img creat imag output img origin img copi input1 output img draw run input1 origin img shape output img shape rais error equal origin img output img equal output img array equal output img array',\n",
              " 'cli logger init src custom node init default parent dir cwd case log captur result cli runner invok cli result exit code captur record get messag cwd parent dir exist parent dir exist cwd exist open cwd infil case dict equal yaml safe load infil',\n",
              " 'record collect record copi deepcopi record record copi get record record record sum copi record len record sum len record',\n",
              " 'rmse correct pred torch tensor repeat targ torch tensor repeat loss equal pred targ']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Preprocessing on the Corpus\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# stop loss words \n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "# punctuation \n",
        "exclude = set(string.punctuation) \n",
        "\n",
        "# lemmatization\n",
        "lemma = WordNetLemmatizer() \n",
        "\n",
        "# One function for all the steps:\n",
        "def clean(doc):\n",
        "    \n",
        "    # convert text into lower case + split into words\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "    \n",
        "    # remove any stop words present\n",
        "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)  \n",
        "    \n",
        "    # remove punctuations + normalize the text\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n",
        "    return normalized\n",
        "\n",
        "# clean data stored in a new list\n",
        "clean_corpus = [clean(doc).split() for doc in corpus]   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D031wO_WfCaq",
        "outputId": "98c43553-4384-4f63-e140-4ffe5bfda20f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxOJCsHrfV-r",
        "outputId": "ecd62479-6bc2-4d39-a043-9f10fd403f23"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['prepar',\n",
              "  'execut',\n",
              "  'eval',\n",
              "  'client',\n",
              "  'eval',\n",
              "  'client',\n",
              "  'client',\n",
              "  'list',\n",
              "  'client',\n",
              "  'proxi',\n",
              "  'fail',\n",
              "  'client',\n",
              "  'success',\n",
              "  'client',\n",
              "  'arr',\n",
              "  'array',\n",
              "  'arr',\n",
              "  'serial',\n",
              "  'ndarray',\n",
              "  'byte',\n",
              "  'arr',\n",
              "  'evalu',\n",
              "  'evalu',\n",
              "  'paramet',\n",
              "  'tensor',\n",
              "  'arr',\n",
              "  'serial',\n",
              "  'tensor',\n",
              "  'type',\n",
              "  'client',\n",
              "  'client',\n",
              "  'result',\n",
              "  'failur',\n",
              "  'evalu',\n",
              "  'client',\n",
              "  'client',\n",
              "  'len',\n",
              "  'result',\n",
              "  'len',\n",
              "  'failur',\n",
              "  'result',\n",
              "  'loss',\n",
              "  'result',\n",
              "  'num',\n",
              "  'exampl'],\n",
              " ['img',\n",
              "  'draw',\n",
              "  'creat',\n",
              "  'imag',\n",
              "  'origin',\n",
              "  'img',\n",
              "  'creat',\n",
              "  'imag',\n",
              "  'output',\n",
              "  'img',\n",
              "  'origin',\n",
              "  'img',\n",
              "  'copi',\n",
              "  'input1',\n",
              "  'output',\n",
              "  'img',\n",
              "  'draw',\n",
              "  'run',\n",
              "  'input1',\n",
              "  'origin',\n",
              "  'img',\n",
              "  'shape',\n",
              "  'output',\n",
              "  'img',\n",
              "  'shape',\n",
              "  'rais',\n",
              "  'error',\n",
              "  'equal',\n",
              "  'origin',\n",
              "  'img',\n",
              "  'output',\n",
              "  'img',\n",
              "  'equal',\n",
              "  'output',\n",
              "  'img',\n",
              "  'array',\n",
              "  'equal',\n",
              "  'output',\n",
              "  'img',\n",
              "  'array'],\n",
              " ['cli',\n",
              "  'logger',\n",
              "  'init',\n",
              "  'src',\n",
              "  'custom',\n",
              "  'node',\n",
              "  'init',\n",
              "  'default',\n",
              "  'parent',\n",
              "  'dir',\n",
              "  'cwd',\n",
              "  'case',\n",
              "  'log',\n",
              "  'captur',\n",
              "  'result',\n",
              "  'cli',\n",
              "  'runner',\n",
              "  'invok',\n",
              "  'cli',\n",
              "  'result',\n",
              "  'exit',\n",
              "  'code',\n",
              "  'captur',\n",
              "  'record',\n",
              "  'get',\n",
              "  'messag',\n",
              "  'cwd',\n",
              "  'parent',\n",
              "  'dir',\n",
              "  'exist',\n",
              "  'parent',\n",
              "  'dir',\n",
              "  'exist',\n",
              "  'cwd',\n",
              "  'exist',\n",
              "  'open',\n",
              "  'cwd',\n",
              "  'infil',\n",
              "  'case',\n",
              "  'dict',\n",
              "  'equal',\n",
              "  'yaml',\n",
              "  'safe',\n",
              "  'load',\n",
              "  'infil'],\n",
              " ['record',\n",
              "  'collect',\n",
              "  'record',\n",
              "  'copi',\n",
              "  'deepcopi',\n",
              "  'record',\n",
              "  'record',\n",
              "  'copi',\n",
              "  'get',\n",
              "  'record',\n",
              "  'record',\n",
              "  'record',\n",
              "  'sum',\n",
              "  'copi',\n",
              "  'record',\n",
              "  'len',\n",
              "  'record',\n",
              "  'sum',\n",
              "  'len',\n",
              "  'record'],\n",
              " ['rmse',\n",
              "  'correct',\n",
              "  'pred',\n",
              "  'torch',\n",
              "  'tensor',\n",
              "  'repeat',\n",
              "  'targ',\n",
              "  'torch',\n",
              "  'tensor',\n",
              "  'repeat',\n",
              "  'loss',\n",
              "  'equal',\n",
              "  'pred',\n",
              "  'targ']]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting text into numerical representation\n",
        "tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
        "\n",
        "# Converting text into numerical representation\n",
        "cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
        "# Array from TF-IDF Vectorizer \n",
        "tf_idf_arr = tf_idf_vectorizer.fit_transform(clean_corpus)\n",
        "\n",
        "# Array from Count Vectorizer \n",
        "cv_arr = cv_vectorizer.fit_transform(clean_corpus)\n",
        "# this is our converted text to numerical representation from the Tf-IDF vectorizer\n",
        "\n",
        "tf_idf_arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXKr8sRyfWup",
        "outputId": "8ea2415b-4399-43ad-ac2e-3a14779ca086"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5x72 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 82 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is our converted text to numerical representation from the Count vectorizer\n",
        "cv_arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdHGsRb_fNxR",
        "outputId": "eea7a490-4d62-4a2d-e34c-fa4fb9bdf80f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5x72 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 82 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating vocabulary array which will represent all the corpus \n",
        "vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n",
        "\n",
        "# get the vocb list\n",
        "vocab_tf_idf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR-t8R96fN7r",
        "outputId": "4c506509-b8c4-4dbf-86e5-8656eeb6a6b2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arr',\n",
              " 'array',\n",
              " 'byte',\n",
              " 'captur',\n",
              " 'case',\n",
              " 'cli',\n",
              " 'client',\n",
              " 'code',\n",
              " 'collect',\n",
              " 'copi',\n",
              " 'correct',\n",
              " 'creat',\n",
              " 'custom',\n",
              " 'cwd',\n",
              " 'deepcopi',\n",
              " 'default',\n",
              " 'dict',\n",
              " 'dir',\n",
              " 'draw',\n",
              " 'equal',\n",
              " 'error',\n",
              " 'eval',\n",
              " 'evalu',\n",
              " 'exampl',\n",
              " 'execut',\n",
              " 'exist',\n",
              " 'exit',\n",
              " 'fail',\n",
              " 'failur',\n",
              " 'get',\n",
              " 'imag',\n",
              " 'img',\n",
              " 'infil',\n",
              " 'init',\n",
              " 'input1',\n",
              " 'invok',\n",
              " 'len',\n",
              " 'list',\n",
              " 'load',\n",
              " 'log',\n",
              " 'logger',\n",
              " 'loss',\n",
              " 'messag',\n",
              " 'ndarray',\n",
              " 'node',\n",
              " 'num',\n",
              " 'open',\n",
              " 'origin',\n",
              " 'output',\n",
              " 'paramet',\n",
              " 'parent',\n",
              " 'pred',\n",
              " 'prepar',\n",
              " 'proxi',\n",
              " 'rais',\n",
              " 'record',\n",
              " 'repeat',\n",
              " 'result',\n",
              " 'rmse',\n",
              " 'run',\n",
              " 'runner',\n",
              " 'safe',\n",
              " 'serial',\n",
              " 'shape',\n",
              " 'src',\n",
              " 'success',\n",
              " 'sum',\n",
              " 'targ',\n",
              " 'tensor',\n",
              " 'torch',\n",
              " 'type',\n",
              " 'yaml']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating vocabulary array which will represent all the corpus \n",
        "vocab_cv = cv_vectorizer.get_feature_names()\n",
        "\n",
        "# get the vocb list\n",
        "vocab_cv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QoFVg65ftCk",
        "outputId": "c6a53291-7cf2-44fb-ebe3-93d9e5ac7584"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arr',\n",
              " 'array',\n",
              " 'byte',\n",
              " 'captur',\n",
              " 'case',\n",
              " 'cli',\n",
              " 'client',\n",
              " 'code',\n",
              " 'collect',\n",
              " 'copi',\n",
              " 'correct',\n",
              " 'creat',\n",
              " 'custom',\n",
              " 'cwd',\n",
              " 'deepcopi',\n",
              " 'default',\n",
              " 'dict',\n",
              " 'dir',\n",
              " 'draw',\n",
              " 'equal',\n",
              " 'error',\n",
              " 'eval',\n",
              " 'evalu',\n",
              " 'exampl',\n",
              " 'execut',\n",
              " 'exist',\n",
              " 'exit',\n",
              " 'fail',\n",
              " 'failur',\n",
              " 'get',\n",
              " 'imag',\n",
              " 'img',\n",
              " 'infil',\n",
              " 'init',\n",
              " 'input1',\n",
              " 'invok',\n",
              " 'len',\n",
              " 'list',\n",
              " 'load',\n",
              " 'log',\n",
              " 'logger',\n",
              " 'loss',\n",
              " 'messag',\n",
              " 'ndarray',\n",
              " 'node',\n",
              " 'num',\n",
              " 'open',\n",
              " 'origin',\n",
              " 'output',\n",
              " 'paramet',\n",
              " 'parent',\n",
              " 'pred',\n",
              " 'prepar',\n",
              " 'proxi',\n",
              " 'rais',\n",
              " 'record',\n",
              " 'repeat',\n",
              " 'result',\n",
              " 'rmse',\n",
              " 'run',\n",
              " 'runner',\n",
              " 'safe',\n",
              " 'serial',\n",
              " 'shape',\n",
              " 'src',\n",
              " 'success',\n",
              " 'sum',\n",
              " 'targ',\n",
              " 'tensor',\n",
              " 'torch',\n",
              " 'type',\n",
              " 'yaml']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(len(vocab_tf_idf))\n",
        "display(len(vocab_cv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "IZx-uD7bftPh",
        "outputId": "d9d6c62c-9787-462f-828f-f1d5a396aed5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Implementation of LDA:\n",
        "    \n",
        "# Create object for the LDA class \n",
        "# Inside this class LDA: define the components:\n",
        "lda_model = LatentDirichletAllocation(n_components = 10, max_iter = 20, random_state = 20)\n",
        "\n",
        "# fit transform on model on our count_vectorizer : running this will return our topics \n",
        "X_topics = lda_model.fit_transform(tf_idf_arr)\n",
        "\n",
        "# .components_ gives us our topic distribution \n",
        "topic_words = lda_model.components_"
      ],
      "metadata": {
        "id": "6jAeWAV6f68A"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Define the number of Words that we want to print in every topic : n_top_words\n",
        "n_top_words = 10\n",
        "\n",
        "for i, topic_dist in enumerate(topic_words):\n",
        "    \n",
        "    # np.argsort to sorting an array or a list or the matrix acc to their values\n",
        "    sorted_topic_dist = np.argsort(topic_dist)\n",
        "    \n",
        "    # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
        "    topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
        "    \n",
        "    # so using the sorted_topic_indexes we ar extracting the words from the vocabulary\n",
        "    # obtaining topics + words\n",
        "    # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
        "    topic_words = topic_words[:-n_top_words:-1]\n",
        "    print (\"Topic\", str(i+1), topic_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqo5PSZzf7Cg",
        "outputId": "da700b9b-f8ae-4205-f1c1-76c57e360caf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1 ['record' 'copi' 'sum' 'len' 'collect' 'deepcopi' 'get' 'array' 'equal']\n",
            "Topic 2 ['cwd' 'exist' 'cli' 'dir' 'parent' 'infil' 'init' 'case' 'captur']\n",
            "Topic 3 ['copi' 'array' 'get' 'equal' 'loss' 'rais' 'error' 'run' 'list']\n",
            "Topic 4 ['copi' 'array' 'get' 'equal' 'loss' 'rais' 'error' 'run' 'list']\n",
            "Topic 5 ['img' 'output' 'origin' 'equal' 'shape' 'draw' 'creat' 'imag' 'input1']\n",
            "Topic 6 ['client' 'arr' 'result' 'evalu' 'failur' 'serial' 'eval' 'len' 'tensor']\n",
            "Topic 7 ['copi' 'array' 'get' 'equal' 'loss' 'rais' 'error' 'run' 'list']\n",
            "Topic 8 ['torch' 'targ' 'pred' 'repeat' 'tensor' 'correct' 'rmse' 'loss' 'equal']\n",
            "Topic 9 ['copi' 'array' 'get' 'equal' 'loss' 'rais' 'error' 'run' 'list']\n",
            "Topic 10 ['copi' 'array' 'get' 'equal' 'loss' 'rais' 'error' 'run' 'list']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To view what topics are assigned to the douments:\n",
        "\n",
        "doc_topic = lda_model.transform(tf_idf_arr)  \n",
        "\n",
        "# iterating over ever value till the end value\n",
        "for n in range(doc_topic.shape[0]):\n",
        "    \n",
        "    # argmax() gives maximum index value\n",
        "    topic_doc = doc_topic[n].argmax()\n",
        "    \n",
        "    # document is n+1  \n",
        "    print (\"Document\", n+1, \" -- Topic:\" ,topic_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekOCu2Hdf7J5",
        "outputId": "0b6e752a-4b9f-4258-ab8e-ae8f877fc5db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1  -- Topic: 5\n",
            "Document 2  -- Topic: 4\n",
            "Document 3  -- Topic: 1\n",
            "Document 4  -- Topic: 0\n",
            "Document 5  -- Topic: 7\n"
          ]
        }
      ]
    }
  ]
}